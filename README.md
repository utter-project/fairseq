# fairseq

This is a fork of the original fairseq repository (version 0.12.2) with added classes for training **mHuBERT-147: A Compact Multilingual HuBERT Model**.

Find details at: https://github.com/utter-project/fairseq/tree/main/examples/mHuBERT-147

## Other resources

* Pre-trained models with manifest files: https://huggingface.co/collections/utter-project/mhubert-147-models-665f1c1dea9a5601a1bfc905

* Pre-processing and clustering scripts: https://github.com/utter-project/mHuBERT-147-scripts

## Citing

```
@inproceedings{boito2024mhubert,
author={Marcely Zanon Boito, Vivek Iyer, Nikolaos Lagos, Laurent Besacier, Ioan Calapodescu},
title={{mHuBERT-147: A Compact Multilingual HuBERT Model}},
year=2024,
booktitle={Interspeech 2024},
}
```

## Funding
<img src="https://github.com/utter-project/fairseq/assets/11521062/cac7ec7f-fd90-4a76-af83-30fc1e5f4d6f" width=20% height=20%> 

This is an output of the European Project UTTER (Unified Transcription and Translation for Extended Reality) funded by European Unionâ€™s Horizon Europe Research and Innovation programme under grant agreement number 101070631.

For more information please visit https://he-utter.eu/
